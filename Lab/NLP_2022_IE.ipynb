{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP Lab - Information Extraction\n",
    "==============================\n",
    "\n",
    "Figure 1 shows the architecture for a simple information extraction system. It begins by processing a document using several procedures: first, the raw text of the document is split into sentences using a sentence segmenter, and each sentence is further subdivided into words using a tokenizer. Next, each sentence is tagged with part-of-speech tags, which will prove very helpful in the next step, named entity detection. In this step, we search for mentions of potentially interesting entities in each sentence. Finally, we use relation detection to search for likely relations between different entities in the text.\n",
    "\n",
    "This lab is based on the Information Extraction chapter of the [NLTK book](http://www.nltk.org/book/).\n",
    "\n",
    "<a href=\"http://www.nltk.org/images/ie-architecture.png\"><img src=\"http://www.nltk.org/images/ie-architecture.png\" width=\"600\" height=\"428\" border=\"0\"></a>\n",
    "Figure 1: Simple Pipeline Architecture for an Information Extraction System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Loading a corpus\n",
    "----------------\n",
    "\n",
    "<b>Example 1</b>\n",
    "\n",
    "This program displays three statistics for each text: average word length, average sentence length, and the number of times each vocabulary item appears in the text on average (our lexical diversity score). Observe that average word length appears to be a general property of English, since it has a recurrent value of 4. (In fact, the average word length is really 3 not 4, since the num_chars variable counts space characters.) By contrast average sentence length and lexical diversity appear to be characteristics of particular authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 25 26 austen-emma.txt\n",
      "5 26 17 austen-persuasion.txt\n",
      "5 28 22 austen-sense.txt\n",
      "4 34 79 bible-kjv.txt\n",
      "5 19 5 blake-poems.txt\n",
      "4 19 14 bryant-stories.txt\n",
      "4 18 12 burgess-busterbrown.txt\n",
      "4 20 13 carroll-alice.txt\n",
      "5 20 12 chesterton-ball.txt\n",
      "5 23 11 chesterton-brown.txt\n",
      "5 18 11 chesterton-thursday.txt\n",
      "4 21 25 edgeworth-parents.txt\n",
      "5 26 15 melville-moby_dick.txt\n",
      "5 52 11 milton-paradise.txt\n",
      "4 12 9 shakespeare-caesar.txt\n",
      "4 12 8 shakespeare-hamlet.txt\n",
      "4 12 7 shakespeare-macbeth.txt\n",
      "5 36 12 whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "#nltk.download('gutenberg')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "for fileid in gutenberg.fileids():\n",
    "    num_chars = len(gutenberg.raw(fileid))\n",
    "    num_words = len(gutenberg.words(fileid))\n",
    "    num_sents = len(gutenberg.sents(fileid))\n",
    "    num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
    "    print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Example 2</b>\n",
    "\n",
    "Load your own corpus, extract sentences and print the number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18407\n"
     ]
    }
   ],
   "source": [
    "corpus_root = 'HerbalMedicines_Corpus/Stockleys'\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*')\n",
    "\n",
    "sentences = wordlists.sents()\n",
    "\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 1</b>\n",
    "\n",
    "Print the 1116th sentence from Macbeth using the Gutenberg corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Double',\n",
       " ',',\n",
       " 'double',\n",
       " ',',\n",
       " 'toile',\n",
       " 'and',\n",
       " 'trouble',\n",
       " ';',\n",
       " 'Fire',\n",
       " 'burne',\n",
       " ',',\n",
       " 'and',\n",
       " 'Cauldron',\n",
       " 'bubble']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "-------------\n",
    "\n",
    "<b>Example 2</b>\n",
    "\n",
    "In this example we will tokenize and tag a sentence with part-of-speech tags then print the annotated sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('will', 'MD'), ('meet', 'VB'), ('John', 'NNP'), ('Smith', 'NNP'), ('to', 'TO'), ('visit', 'VB'), ('Oracle', 'NNP'), ('headquarters', 'NNS'), ('tomorrow', 'NN'), ('morning', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "sentence = \"I will meet John Smith to visit Oracle headquarters tomorrow morning.\";\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print (pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load sentences from one of the categories of the Brown Corpus. To do this, first we display all the categories from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "#nltk.download('brown')\n",
    "\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 2</b>\n",
    "\n",
    "POS tag the first three sentences of the Brown Corpus that are in the category '<i>news</i>' and then print the original sentences and the tagged sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0:\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
      "\n",
      "\n",
      "Sentence with POS:\n",
      "[('The', 'DT'), ('Fulton', 'NNP'), ('County', 'NNP'), ('Grand', 'NNP'), ('Jury', 'NNP'), ('said', 'VBD'), ('Friday', 'NNP'), ('an', 'DT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NNP'), ('recent', 'JJ'), ('primary', 'JJ'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'DT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'IN'), ('any', 'DT'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]\n",
      "\n",
      "\n",
      "Sentence 1:\n",
      "['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']\n",
      "\n",
      "\n",
      "Sentence with POS:\n",
      "[('The', 'DT'), ('jury', 'NN'), ('further', 'RB'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'JJ'), ('presentments', 'NNS'), ('that', 'IN'), ('the', 'DT'), ('City', 'NNP'), ('Executive', 'NNP'), ('Committee', 'NNP'), (',', ','), ('which', 'WDT'), ('had', 'VBD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'DT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'DT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('City', 'NNP'), ('of', 'IN'), ('Atlanta', 'NNP'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'DT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'DT'), ('election', 'NN'), ('was', 'VBD'), ('conducted', 'VBN'), ('.', '.')]\n",
      "\n",
      "\n",
      "Sentence 2:\n",
      "['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.']\n",
      "\n",
      "\n",
      "Sentence with POS:\n",
      "[('The', 'DT'), ('September-October', 'NNP'), ('term', 'NN'), ('jury', 'NN'), ('had', 'VBD'), ('been', 'VBN'), ('charged', 'VBN'), ('by', 'IN'), ('Fulton', 'NNP'), ('Superior', 'NNP'), ('Court', 'NNP'), ('Judge', 'NNP'), ('Durwood', 'NNP'), ('Pye', 'NNP'), ('to', 'TO'), ('investigate', 'VB'), ('reports', 'NNS'), ('of', 'IN'), ('possible', 'JJ'), ('``', '``'), ('irregularities', 'NNS'), (\"''\", \"''\"), ('in', 'IN'), ('the', 'DT'), ('hard-fought', 'JJ'), ('primary', 'NN'), ('which', 'WDT'), ('was', 'VBD'), ('won', 'VBN'), ('by', 'IN'), ('Mayor-nominate', 'NNP'), ('Ivan', 'NNP'), ('Allen', 'NNP'), ('Jr.', 'NNP'), ('.', '.')]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "sentences=brown.sents(categories='news')\n",
    "\n",
    "for i in range(0, 3):\n",
    "    # Print the original sentence\n",
    "    print (\"Sentence \" + str(i) + \":\")\n",
    "    sent = sentences[i]\n",
    "    print (sent)\n",
    "    print (\"\\n\")\n",
    "\n",
    "    # Print the tagged sentence with POS\n",
    "    print (\"Sentence with POS:\")\n",
    "    # Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking\n",
    "--------\n",
    "\n",
    "Chunking is a basic technique used for entity detection is chunking, which segments and labels multi-token sequences as illustrated in Figure 2.\n",
    "\n",
    "<a href=\"http://www.nltk.org/images/chunk-segmentation.png\"><img src=\"http://www.nltk.org/images/chunk-segmentation.png\" width=\"600\" height=\"428\" border=\"0\"></a>\n",
    "\n",
    "Figure 2: Segmentation and Labeling at both the Token and Chunk Levels\n",
    "\n",
    "To find the chunk structure for a given sentence, the RegexpParser chunker begins with a flat structure in which no tokens are chunked. The chunking rules are applied in turn, successively updating the chunk structure. Once all of the rules have been invoked, the resulting chunk structure is returned.\n",
    "\n",
    "<b>Example 3:</b> Noun Phrase Chunking\n",
    "\n",
    "In order to create an NP-chunker, we will first define a chunk grammar, consisting of rules that indicate how sentences should be chunked. In this case, we will define a simple grammar with a single regular-expression rule. This rule says that an NP chunk should be formed whenever the chunker finds an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN). Using this grammar, we create a chunk parser, and test it on our example sentence. The result is a tree, which we can either print, or display graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  Fulton/NNP\n",
      "  County/NNP\n",
      "  Grand/NNP\n",
      "  Jury/NNP\n",
      "  said/VBD\n",
      "  Friday/NNP\n",
      "  (NP an/DT investigation/NN)\n",
      "  of/IN\n",
      "  Atlanta's/NNP\n",
      "  (NP recent/JJ primary/JJ election/NN)\n",
      "  produced/VBD\n",
      "  ``/``\n",
      "  (NP no/DT evidence/NN)\n",
      "  ''/''\n",
      "  that/IN\n",
      "  any/DT\n",
      "  irregularities/NNS\n",
      "  took/VBD\n",
      "  (NP place/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP The/DT jury/NN)\n",
      "  further/RB\n",
      "  said/VBD\n",
      "  in/IN\n",
      "  term-end/JJ\n",
      "  presentments/NNS\n",
      "  that/IN\n",
      "  the/DT\n",
      "  City/NNP\n",
      "  Executive/NNP\n",
      "  Committee/NNP\n",
      "  ,/,\n",
      "  which/WDT\n",
      "  had/VBD\n",
      "  (NP over-all/JJ charge/NN)\n",
      "  of/IN\n",
      "  (NP the/DT election/NN)\n",
      "  ,/,\n",
      "  ``/``\n",
      "  deserves/VBZ\n",
      "  (NP the/DT praise/NN)\n",
      "  and/CC\n",
      "  thanks/NNS\n",
      "  of/IN\n",
      "  the/DT\n",
      "  City/NNP\n",
      "  of/IN\n",
      "  Atlanta/NNP\n",
      "  ''/''\n",
      "  for/IN\n",
      "  (NP the/DT manner/NN)\n",
      "  in/IN\n",
      "  which/WDT\n",
      "  (NP the/DT election/NN)\n",
      "  was/VBD\n",
      "  conducted/VBN\n",
      "  ./.)\n",
      "(S\n",
      "  The/DT\n",
      "  September-October/NNP\n",
      "  (NP term/NN)\n",
      "  (NP jury/NN)\n",
      "  had/VBD\n",
      "  been/VBN\n",
      "  charged/VBN\n",
      "  by/IN\n",
      "  Fulton/NNP\n",
      "  Superior/NNP\n",
      "  Court/NNP\n",
      "  Judge/NNP\n",
      "  Durwood/NNP\n",
      "  Pye/NNP\n",
      "  to/TO\n",
      "  investigate/VB\n",
      "  reports/NNS\n",
      "  of/IN\n",
      "  possible/JJ\n",
      "  ``/``\n",
      "  irregularities/NNS\n",
      "  ''/''\n",
      "  in/IN\n",
      "  (NP the/DT hard-fought/JJ primary/NN)\n",
      "  which/WDT\n",
      "  was/VBD\n",
      "  won/VBN\n",
      "  by/IN\n",
      "  Mayor-nominate/NNP\n",
      "  Ivan/NNP\n",
      "  Allen/NNP\n",
      "  Jr./NNP\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\" # simple grammar\n",
    "cp = nltk.RegexpParser(grammar) # create a chunk parser using this grammar\n",
    "\n",
    "sentences=brown.sents(categories='news')\n",
    "\n",
    "for i in range(0, 3):\n",
    "    sent = nltk.pos_tag(sentences[i])\n",
    "    result = cp.parse(sent) # the result is a tree\n",
    "    \n",
    "    print (result) # print the tree\n",
    "    \n",
    "    #result.draw() # draw graphically\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 3:</b> Verb Phrase Chunking\n",
    "\n",
    "Create a verb phrase chunker for the following pattern \"verb to verb\" that covers verb phrases such as \"serve to protect\" and \"like to see\". Run this chunker on the first 100 sentences of the Brown Corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CHUNK combined/VBN to/TO achieve/VB)\n",
      "(CHUNK continue/VB to/TO place/VB)\n",
      "(CHUNK serve/VB to/TO protect/VB)\n",
      "(CHUNK wanted/VBD to/TO wait/VB)\n",
      "(CHUNK allowed/VBN to/TO place/VB)\n",
      "(CHUNK expected/VBN to/TO become/VB)\n",
      "(CHUNK expected/VBN to/TO approve/VB)\n",
      "(CHUNK expected/VBN to/TO make/VB)\n",
      "(CHUNK intends/VBZ to/TO make/VB)\n",
      "(CHUNK seek/VB to/TO set/VB)\n",
      "(CHUNK like/VB to/TO see/VB)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Your code goes here\n",
    "grammar = \"CHUNK: {}\"\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "brown = nltk.corpus.brown\n",
    "sentences = brown.tagged_sents()\n",
    "\n",
    "for i in range(0, 100):\n",
    "     tree = cp.parse(sentences[i])\n",
    "     for subtree in tree.subtrees():\n",
    "         if subtree.label() == 'CHUNK': print(subtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition\n",
    "------------------------\n",
    "\n",
    "NLTK provides a classifier that has already been trained to recognize named entities, accessed with the function nltk.ne_chunk(). If we set the parameter binary=True [1], then named entities are just tagged as NE; otherwise, the classifier adds category labels such as PERSON, ORGANIZATION, and GPE. \n",
    "\n",
    "<b>Example 4</b>\n",
    "\n",
    "Annotate a sentence with named entities and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  will/MD\n",
      "  meet/VB\n",
      "  (PERSON John/NNP Smith/NNP)\n",
      "  to/TO\n",
      "  visit/VB\n",
      "  (GPE Oracle/NNP)\n",
      "  headquarters/NNS\n",
      "  tomorrow/NN\n",
      "  morning/NN)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/gb5/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/gb5/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "sentence = \"I will meet John Smith to visit Oracle headquarters tomorrow morning\";\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "print (nltk.ne_chunk(pos_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 4</b>\n",
    "\n",
    "Extract all the unique named entities from the first 20 sentences of the of the Brown Corpus that are in the category '<i>news</i>'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fulton County Grand Jury']\n",
      "['City Executive Committee', 'Atlanta']\n",
      "['Fulton Superior Court']\n",
      "[]\n",
      "[]\n",
      "['Fulton']\n",
      "['Atlanta', 'Fulton County']\n",
      "['Merger']\n",
      "[]\n",
      "['City Purchasing Department']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['Fulton County', 'Fulton County']\n",
      "[]\n",
      "['Fulton County']\n",
      "['Fulton']\n",
      "['Fulton']\n",
      "[]\n",
      "['Fulton County Grand Jury', 'City Executive Committee', 'Atlanta', 'Fulton Superior Court', 'Fulton', 'Atlanta', 'Fulton County', 'Merger', 'City Purchasing Department', 'Fulton County', 'Fulton County', 'Fulton County', 'Fulton', 'Fulton']\n",
      "{'Merger', 'City Purchasing Department', 'City Executive Committee', 'Atlanta', 'Fulton County Grand Jury', 'Fulton County', 'Fulton Superior Court', 'Fulton'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "def extract_entity_names(t):\n",
    "    # Your code goes here\n",
    "    \n",
    "    return entity_names\n",
    "\n",
    "news_sentences=brown.sents(categories='news')\n",
    "\n",
    "entity_names = []\n",
    "\n",
    "for i in range(0, 20):\n",
    "  tagged_sent = nltk.pos_tag(news_sentences[i])\n",
    "  chunked_sent = nltk.ne_chunk(tagged_sent, binary=True)\n",
    "\n",
    "  # Print results per sentence\n",
    "  print (extract_entity_names(chunked_sent)) \n",
    "  entity_names.extend(extract_entity_names(chunked_sent))\n",
    "\n",
    "# Print all entity names\n",
    "print (entity_names)\n",
    "\n",
    "# Print unique entity names\n",
    "print (set(entity_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relation Extraction\n",
    "-------------------\n",
    "\n",
    "Once named entities have been identified in a text, we then want to extract the relations that exist between them. We will typically be looking for relations between specified types of named entity. One way of approaching this task is to initially look for all triples of the form (X, α, Y), where X and Y are named entities of the required types, and α is the string of words that intervenes between X and Y. We can then use regular expressions to pull out just those instances of α that express the relation that we are looking for. \n",
    "\n",
    "<b>Example 5</b>\n",
    "\n",
    "The following example searches for strings that contain the word in. The special regular expression (?!\\b.+ing\\b) is a negative lookahead assertion that allows us to disregard strings such as <i>success in supervising the transition of</i>, where <i>in</i> is followed by a gerund. For this example we will use the nltk.corpus.ieer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package ieer to /home/gb5/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n",
      "[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n",
      "[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n",
      "[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n",
      "[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n",
      "[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n",
      "[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n",
      "[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n",
      "[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n",
      "[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n",
      "[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n",
      "[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n",
      "[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/ieer.zip.\n"
     ]
    }
   ],
   "source": [
    "import re, nltk\n",
    "\n",
    "nltk.download('ieer')\n",
    "\n",
    "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing\\b)')\n",
    "        \n",
    "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
    "    for rel in nltk.sem.relextract.extract_rels('ORG', 'LOC', doc,corpus='ieer', pattern = IN):\n",
    "         print (nltk.sem.relextract.rtuple(rel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 5</b>\n",
    "\n",
    "Extract places of birth of people from the the ieeer corpus, using the 'X born in Y' pattern, where X is a person and Y is a location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PER: 'McCarthy'] 'was born in' [LOC: 'Belle Plaine']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import ieer\n",
    "\n",
    "# Your code goes here\n",
    "#BORNIN = re.compile('')\n",
    "\n",
    "for fileId in ieer.fileids():\n",
    "    for doc in nltk.corpus.ieer.parsed_docs(fileId):\n",
    "         for rel in nltk.sem.relextract.extract_rels('PER', 'LOC', doc, corpus='ieer', pattern = BORNIN):\n",
    "                print(nltk.sem.relextract.rtuple(rel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Example 6</b>\n",
    "\n",
    "In this example we extract people and their role in an organisation using a predefined list of roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IEER: has_role(PER, ORG) -- raw rtuples:\n",
      "=============================================\n",
      "[PER: 'Kivutha Kibwana'] ', of the' [ORG: 'National Convention Assembly']\n",
      "[PER: 'Boban Boskovic'] ', chief executive of the' [ORG: 'Plastika']\n",
      "[PER: 'Annan'] ', the first sub-Saharan African to head the' [ORG: 'United Nations']\n",
      "[PER: 'Kiriyenko'] 'became a foreman at the' [ORG: 'Krasnoye Sormovo']\n",
      "[PER: 'Annan'] ', the first sub-Saharan African to head the' [ORG: 'United Nations']\n",
      "[PER: 'Mike Godwin'] ', chief counsel for the' [ORG: 'Electronic Frontier Foundation']\n",
      "[PER: 'Robert Mergess'] ', the co-director of the' [ORG: 'Berkeley Center for Law and Technology']\n",
      "[PER: 'Jack Balkin'] \", director of the school's program. ``What happened at\" [ORG: 'Yale']\n",
      "[PER: 'William Gale'] ', an economist at the' [ORG: 'Brookings Institution']\n",
      "[PER: 'Joel Slemrod'] ', an economist at the' [ORG: 'University of Michigan']\n",
      "[PER: 'Alan Braverman'] ', Internet analyst at' [ORG: 'Credit Suisse First Boston']\n",
      "[PER: 'Michael Coffey'] ', managing editor of' [ORG: 'Publishers Weekly']\n",
      "[PER: 'Lorne Michaels'] \", the executive producer of ``Saturday Night Live.''\" [ORG: 'TV Books']\n",
      "[PER: 'James Billington'] ', the librarian of' [ORG: 'Congress']\n",
      "[PER: 'Sherry Lansing'] ', chairwoman of the' [ORG: 'Paramount Motion Picture Group']\n",
      "[PER: 'Charlotte Forest'] ', executive producer for' [ORG: 'Homestead Editorial']\n",
      "[PER: 'John Wren'] ', the president and chief executive at' [ORG: 'Omnicom']\n",
      "[PER: 'Charlie Moss'] ', the chairman of' [ORG: 'Moss/Dragoti']\n",
      "[PER: 'Norio Ohga'] ', chairman of the' [ORG: 'Sony Corporation']\n",
      "[PER: 'Wendy Beale Needham'] ', an auto analyst at' [ORG: 'Donaldson, Lufkin &AMP; Jenrette']\n",
      "[PER: 'David Bradley'] ', an auto analyst at' [ORG: 'J.P. Morgan Securities Inc.']\n",
      "[PER: 'Michael W. McCarthy'] ', a former chairman of' [ORG: 'Merrill Lynch &AMP; Co.']\n",
      "[PER: 'McCarthy'] 'was a governor of the' [ORG: 'American Stock Exchange']\n",
      "[PER: 'Anthony Chan'] ', managing director and chief economist at the' [ORG: 'Banc One Investment Advisors Corp.']\n",
      "[PER: 'Paul Volcker'] ', former chairman of the' [ORG: 'U.S. Federal Reserve']\n",
      "[PER: 'Katherine Abraham'] ', commissioner of the' [ORG: 'Bureau of Labor Statistics']\n",
      "[PER: 'David Wyss'] ', chief economist at' [ORG: \"Standard &AMP; Poor's DRI\"]\n",
      "[PER: 'Bruce Steinberg'] ', chief economist at' [ORG: 'Merrill Lynch']\n",
      "[PER: 'Mitchell Fromstein'] ', head of' [ORG: 'Manpower Inc.']\n",
      "[PER: 'Barry Travis'] ', executive director of the' [ORG: 'Little Rock Convention and Visitors Bureau']\n",
      "[PER: 'Pamela Bethel'] ', a lawyer for' [ORG: 'Kramerbooks']\n",
      "[PER: 'Walter'] ', who now heads the' [ORG: 'Waterford Foundation']\n",
      "[PER: 'Alan Horn'] ', the chairman of' [ORG: 'Castle Rock Entertainment']\n",
      "[PER: 'Dan Morgenstern'] ', director of the' [ORG: 'Institute of Jazz Studies']\n",
      "[PER: 'Robert Schelin'] ', acting managing director of' [ORG: 'Smithsonian Press/Smithsonian Productions']\n",
      "[PER: 'Peter Cannell'] ', director of the' [ORG: 'Smithsonian Institution Press']\n",
      "[PER: 'Trimble'] ', the leader of the Protestant' [ORG: 'Ulster Unionist Party']\n",
      "[PER: 'Seamus Mallon'] ', deputy leader of the' [ORG: 'Social Democrats']\n",
      "[PER: 'Alexander Shokhin'] ', the parliamentary leader of' [ORG: 'Our Home Is Russia']\n",
      "[PER: 'Yegor Stroyev'] ', the head of the' [ORG: 'upper house']\n"
     ]
    }
   ],
   "source": [
    "def roles_demo(trace=0):\n",
    "    from nltk.corpus import ieer\n",
    "    roles = \"\"\"\n",
    "    (.*(                   # assorted roles\n",
    "    analyst|\n",
    "    chair(wo)?man|\n",
    "    commissioner|\n",
    "    counsel|\n",
    "    director|\n",
    "    economist|       \n",
    "    editor|\n",
    "    executive|\n",
    "    foreman|\n",
    "    governor|\n",
    "    head|\n",
    "    lawyer|\n",
    "    leader|\n",
    "    librarian).*)|\n",
    "    manager|\n",
    "    partner|\n",
    "    president|\n",
    "    producer|\n",
    "    professor|\n",
    "    researcher|\n",
    "    spokes(wo)?man|\n",
    "    writer|\n",
    "    ,\\sof\\sthe?\\s*  # \"X, of (the) Y\"\n",
    "    \"\"\"\n",
    "    ROLES = re.compile(roles, re.VERBOSE)\n",
    "\n",
    "    print()\n",
    "    print(\"IEER: has_role(PER, ORG) -- raw rtuples:\")\n",
    "    print(\"=\" * 45)\n",
    "\n",
    "    for file in ieer.fileids():\n",
    "        for doc in ieer.parsed_docs(file):\n",
    "            lcon = rcon = False\n",
    "            if trace:\n",
    "                print(doc.docno)\n",
    "                print(\"=\" * 15)\n",
    "                lcon = rcon = True\n",
    "            for rel in nltk.sem.relextract.extract_rels('PER', 'ORG', doc, corpus='ieer', pattern=ROLES):\n",
    "                print(nltk.sem.relextract.rtuple(rel, lcon=lcon, rcon=rcon))\n",
    "\n",
    "roles_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 6</b>\n",
    "\n",
    "Extract people and their role in an organisation by using the 'X ROLE at the Y' or 'X ROLE of the Y' patterns, where X is a person and Y is an organisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PER: 'Kivutha Kibwana'] ', of the' [ORG: 'National Convention Assembly']\n",
      "[PER: 'Boban Boskovic'] ', chief executive of the' [ORG: 'Plastika']\n",
      "[PER: 'Robert Mergess'] ', the co-director of the' [ORG: 'Berkeley Center for Law and Technology']\n",
      "[PER: 'Jack Balkin'] \", director of the school's program. ``What happened at\" [ORG: 'Yale']\n",
      "[PER: 'David Post'] ', co-founder of the' [ORG: 'Cyberspace Law Institute']\n",
      "[PER: 'William Gale'] ', an economist at the' [ORG: 'Brookings Institution']\n",
      "[PER: 'Joel Slemrod'] ', an economist at the' [ORG: 'University of Michigan']\n",
      "[PER: 'Kaufman'] ', president of the privately held' [ORG: 'TV Books LLC']\n",
      "[PER: 'Sherry Lansing'] ', chairwoman of the' [ORG: 'Paramount Motion Picture Group']\n",
      "[PER: 'Rick Yorn'] ', his manager at the firm' [ORG: 'Addis-Wechsler &AMP; Associates']\n",
      "[PER: 'Ken Kaess'] ', president of the' [ORG: 'DDB Needham']\n",
      "[PER: 'Norio Ohga'] ', chairman of the' [ORG: 'Sony Corporation']\n",
      "[PER: 'Raymond Rosen'] ', a sex researcher and professor of psychiatry at the' [ORG: 'Robert Wood Johnson Medical School']\n",
      "[PER: 'Pepper Schwartz'] ', a sociology professor at the' [ORG: 'University of Washington']\n",
      "[PER: 'Irwin Goldstein'] ', a professor of urology at the' [ORG: 'Boston University School of Medicine']\n",
      "[PER: 'Jennifer Berman'] ', a urologist at the' [ORG: 'University of Maryland']\n",
      "[PER: 'Anthony Chan'] ', managing director and chief economist at the' [ORG: 'Banc One Investment Advisors Corp.']\n",
      "[PER: 'Kevin Ashby'] ', publisher of the local newspaper,' [ORG: 'The Sun Advocate']\n",
      "[PER: 'Paul Volcker'] ', former chairman of the' [ORG: 'U.S. Federal Reserve']\n",
      "[PER: 'Israel Singer'] ', the secretary-general of the' [ORG: 'World Jewish Congress']\n",
      "[PER: 'Katherine Abraham'] ', commissioner of the' [ORG: 'Bureau of Labor Statistics']\n",
      "[PER: 'Lloyd Kiva New'] ', president emeritus of the' [ORG: 'Institute of American Indian Art']\n",
      "[PER: 'Barry Travis'] ', executive director of the' [ORG: 'Little Rock Convention and Visitors Bureau']\n",
      "[PER: 'Linda Ward'] ', general manager of the' [ORG: 'Legacy Hotel']\n",
      "[PER: 'J. Jackson Walter'] ', who served as president of the' [ORG: 'National Trust for Historic Preservation']\n",
      "[PER: 'Dan Morgenstern'] ', director of the' [ORG: 'Institute of Jazz Studies']\n",
      "[PER: 'Peter Cannell'] ', director of the' [ORG: 'Smithsonian Institution Press']\n",
      "[PER: 'Trimble'] ', the leader of the Protestant' [ORG: 'Ulster Unionist Party']\n",
      "[PER: 'Seamus Mallon'] ', deputy leader of the' [ORG: 'Social Democrats']\n",
      "[PER: 'Yegor Stroyev'] ', the head of the' [ORG: 'upper house']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import ieer\n",
    "\n",
    "# Your code goes here\n",
    "#ROLES = re.compile('')\n",
    "\n",
    "for file in ieer.fileids():\n",
    "    for doc in nltk.corpus.ieer.parsed_docs(file):\n",
    "        for r in nltk.sem.relextract.extract_rels('PER', 'ORG', doc, corpus='ieer', pattern=ROLES):\n",
    "            #print  (nltk.sem.relextract.clause(r, relsym=\"ROLES\"))\n",
    "            print (nltk.sem.relextract.rtuple(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The WordNet hierarchy\n",
    "---------------------\n",
    "\n",
    "WordNet is a semantically-oriented dictionary of English, similar to a traditional thesaurus but with a richer structure. NLTK includes the English WordNet, with 155,287 words and 117,659 synonym sets. Concepts are hierarchically organised using hypernym-hyponym relations.\n",
    "\n",
    "<b>Example 7</b>\n",
    "\n",
    "Given a concept like feline, we can look at the concepts that are more specific; the (immediate) hyponyms. We can also navigate up the hierarchy by visiting hypernyms. Some words have multiple paths, because they can be classified in more than one way. There are two paths between car.n.01 and entity.n.01 because wheeled_vehicle.n.01 can be classified as both a vehicle and a container. Some words have multiple paths, because they can be classified in more than one way. There are two paths between car.n.01 and entity.n.01 because wheeled_vehicle.n.01 can be classified as both a vehicle and a container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/gb5/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['eutherian', 'eutherian_mammal', 'placental', 'placental_mammal']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "carnivore = wn.synset('carnivore.n.01')\n",
    "carnivore_isa = carnivore.hypernyms()\n",
    "carnivore_isa[0]\n",
    "sorted(lemma.name() for synset in carnivore_isa for lemma in synset.lemmas())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 7</b>\n",
    "\n",
    "List the concepts that are more specific than the noun carnivore, in other words its (immediate) hyponyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bear',\n",
       " 'canid',\n",
       " 'canine',\n",
       " 'felid',\n",
       " 'feline',\n",
       " 'fissiped',\n",
       " 'fissiped_mammal',\n",
       " 'mustelid',\n",
       " 'musteline',\n",
       " 'musteline_mammal',\n",
       " 'procyonid',\n",
       " 'viverrine',\n",
       " 'viverrine_mammal']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "carnivore = wn.synset('carnivore.n.01')\n",
    "\n",
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Example 8</b>\n",
    "\n",
    "Some words have multiple paths, because they can be classified in more than one way. There are two paths between car.n.01 and entity.n.01 because wheeled_vehicle.n.01 can be classified as both a vehicle and a container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entity.n.01',\n",
       " 'physical_entity.n.01',\n",
       " 'object.n.01',\n",
       " 'whole.n.02',\n",
       " 'artifact.n.01',\n",
       " 'instrumentality.n.03',\n",
       " 'conveyance.n.03',\n",
       " 'vehicle.n.01',\n",
       " 'wheeled_vehicle.n.01',\n",
       " 'self-propelled_vehicle.n.01',\n",
       " 'motor_vehicle.n.01',\n",
       " 'car.n.01']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car=wn.synset('car.n.01')\n",
    "types_of_car=car.hypernyms()\n",
    "paths = car.hypernym_paths()\n",
    "len(paths) # display the number of paths to root\n",
    "\n",
    "#[synset.name() for synset in paths[0]] #display the first path\n",
    "\n",
    "[synset.name() for synset in paths[1]] #display the second path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 9/b>\n",
    "Read the Stockleys corpus extracted from Stockley’s Herbal Medicines Interactions and extract pairs of hyponyms and hypernyms using the rules provided in the lecture. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
